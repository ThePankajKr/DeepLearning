{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "8FEbLPCGBDvH"
      },
      "outputs": [],
      "source": [
        "#So first of all we are going to import all the libraries\n",
        "#numpy,dataset from keras, padding, sequential model, Lstm,dense\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load DAtasets\n",
        "\n",
        "vocab_size = 30000 #top 10k words only to reduce complexity\n",
        "maxlen = 200 #max length of each review\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "\n",
        "#Now we will pad sequences so that all the reviews have the same reviews\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen = maxlen )\n",
        "x_test = pad_sequences(X_test, maxlen = maxlen )"
      ],
      "metadata": {
        "id": "iE1cCfqMCKRb"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's Build our Lstm model"
      ],
      "metadata": {
        "id": "THgvCli0Di11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential ([\n",
        "    #word embedding\n",
        "    Embedding(input_dim= vocab_size, output_dim =128, input_length = maxlen),\n",
        "    #LSTM layer\n",
        "    LSTM(128, return_sequences = False ),\n",
        "    Dropout(0.5),\n",
        "    #binary classification\n",
        "    Dense(1,activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "uNQjCbKwDbje"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 4: Compile Model\n",
        "\n",
        "model.compile(loss= 'binary_crossentropy',\n",
        "              optimizer = 'adam',\n",
        "              metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "T-SWz9CBEnVp"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 5: Let's train our model\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs = 5, batch_size =64, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjGhbYE9Ex9S",
        "outputId": "82fd150b-ddfe-463e-fc15-72ffa470c451"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 541ms/step - accuracy: 0.7137 - loss: 0.5357 - val_accuracy: 0.8596 - val_loss: 0.3297\n",
            "Epoch 2/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 537ms/step - accuracy: 0.9161 - loss: 0.2275 - val_accuracy: 0.8618 - val_loss: 0.3693\n",
            "Epoch 3/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 534ms/step - accuracy: 0.9543 - loss: 0.1342 - val_accuracy: 0.8734 - val_loss: 0.3716\n",
            "Epoch 4/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 540ms/step - accuracy: 0.9391 - loss: 0.1626 - val_accuracy: 0.8662 - val_loss: 0.4005\n",
            "Epoch 5/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 536ms/step - accuracy: 0.9749 - loss: 0.0758 - val_accuracy: 0.8676 - val_loss: 0.5117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 6 : Now it's time to evaluate on test data\n",
        "\n",
        "loss, acc = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {acc*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxGXt6qoFmYX",
        "outputId": "cdc9c584-9ad1-4979-c1a6-24da0a754512"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 105ms/step - accuracy: 0.8443 - loss: 0.5916\n",
            "Test Accuracy: 84.43%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = {value: key for key, value in word_index.items()}\n",
        "\n",
        "#Example review(covert words -> integers)\n",
        "\n",
        "def encode_review(text):\n",
        "  tokens = [word_index.get(word, 2) for word in text.lower().split()]\n",
        "  return pad_sequences ([tokens], maxlen= maxlen)\n",
        "\n",
        "sample_review = \"This movie was bad  \"\n",
        "encoded = encode_review(sample_review)\n",
        "prediction = model.predict(encoded)\n",
        "print(\"Positive\" if prediction [0][0]> 0.5 else \"Negative\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETunEwCvF-5K",
        "outputId": "a5a509dc-e046-4591-b4c9-4be82468b349"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 16 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f1c32f779c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step\n",
            "Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = {value: key for key, value in word_index.items()}\n",
        "\n",
        "def encode_review(text):\n",
        "    tokens = [word_index.get(word, 2) for word in text.lower().split()]  # 2 = <UNK>\n",
        "    return pad_sequences([tokens], maxlen=200)\n",
        "\n",
        "review = \"This movie was very good\"\n",
        "encoded = encode_review(review)\n",
        "prediction = model.predict(encoded)\n",
        "print(\"Positive\" if prediction[0][0] > 0.5 else \"Negative\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eCaqbeQJZ7l",
        "outputId": "fd5a802a-32a9-4746-ceed-1625db729ba0"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(x_test[:20])\n",
        "print(preds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fz_xsXZdJ7X8",
        "outputId": "ff730700-7ed2-4d99-cc8b-1d9090ca2153"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f1c32f779c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step\n",
            "[[4.5153070e-02]\n",
            " [9.9912280e-01]\n",
            " [7.1210468e-01]\n",
            " [5.8373413e-03]\n",
            " [9.9988085e-01]\n",
            " [8.4859401e-01]\n",
            " [9.9799532e-01]\n",
            " [1.2022696e-03]\n",
            " [9.9416435e-01]\n",
            " [9.9957258e-01]\n",
            " [9.9607146e-01]\n",
            " [1.4120619e-03]\n",
            " [4.3119340e-05]\n",
            " [8.5139228e-04]\n",
            " [9.9816543e-01]\n",
            " [3.1683318e-04]\n",
            " [9.9971741e-01]\n",
            " [4.1807536e-02]\n",
            " [1.8256143e-05]\n",
            " [2.6391740e-03]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = imdb.get_word_index()\n",
        "print(\"bad\" in word_index)   # True/False\n",
        "print(\"good\" in word_index)\n",
        "print(\"boring\" in word_index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePC2Azo0J_1f",
        "outputId": "64e2ea42-badc-4ace-ce13-6ae5c9aeaed4"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token=\"<UNK>\")\n",
        "tokenizer.fit_on_texts([\"This movie was bad\", \"I love this movie\"])  # or full dataset\n",
        "\n",
        "seq = tokenizer.texts_to_sequences([\"This movie was bad\"])\n",
        "padded = pad_sequences(seq, maxlen=200)\n",
        "pred = model.predict(padded)\n",
        "print(pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBvNFzoaKQz_",
        "outputId": "c839c193-96de-4a7f-ba7b-c44b247ad336"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "[[0.4513538]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cITNz2qEKVP8"
      },
      "execution_count": 51,
      "outputs": []
    }
  ]
}